{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c81296ae-1c80-409b-a322-9e33290c84ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_excel(\"51journals_UTD_FT_openalexWOSdoi_utdft.xlsx\")\n",
    "\n",
    "# Split affiliations\n",
    "def split_affils(val):\n",
    "    if pd.isna(val):\n",
    "        return []\n",
    "    return [x.strip() for x in val.split(\";\") if x.strip()]\n",
    "\n",
    "df[\"affil_list\"] = df[\"affiliations\"].apply(split_affils)\n",
    "\n",
    "# Split authors\n",
    "def split_authors(val):\n",
    "    if pd.isna(val):\n",
    "        return []\n",
    "    return [x.strip() for x in val.split(\";\") if x.strip()]\n",
    "\n",
    "df[\"author_list\"] = df[\"authors\"].apply(split_authors)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# LONG FORMAT: Paper × University\n",
    "# ----------------------------------------------------\n",
    "rows_univ = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    unique_unis = set(row[\"affil_list\"])  # unique universities per paper\n",
    "    \n",
    "    for uni in unique_unis:\n",
    "        rows_univ.append({\n",
    "            \"paper_id\": int(idx),\n",
    "            \"title\": row[\"title\"],\n",
    "            \"year\": int(row[\"year\"]),\n",
    "            \"journal\": row[\"journalFullName\"],\n",
    "            \"university\": uni,\n",
    "            \"disciplineAbbr\": row[\"disciplineAbbr\"],\n",
    "            \"utd24\": int(row[\"utd24\"]),\n",
    "            \"ft50\": int(row[\"ft50\"])\n",
    "        })\n",
    "\n",
    "df_univ = pd.DataFrame(rows_univ)\n",
    "\n",
    "# ================================\n",
    "# APPLY STATA CLEANING RULES HERE\n",
    "# ================================\n",
    "\n",
    "# Trim internal multiple spaces\n",
    "df_univ[\"university\"] = df_univ[\"university\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# REPLACEMENTS (same as STATA)\n",
    "# ----------------------------------------------------\n",
    "replace_map = {\n",
    "    \"Indiana University Bloomington\": \"IU Kelley School of Business\",\n",
    "    \"Universite de Montreal\": \"HEC Montreal\",\n",
    "    \"IESE Business School\": \"University of Navarra\"\n",
    "}\n",
    "df_univ[\"university\"] = df_univ[\"university\"].replace(replace_map)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# DROP duplicates per paper (same as: by ut aff: keep if _n==_N)\n",
    "# ----------------------------------------------------\n",
    "df_univ = df_univ.drop_duplicates(subset=[\"paper_id\", \"university\"], keep=\"last\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# FLAG: University Systems (univSystem == 1)\n",
    "# ----------------------------------------------------\n",
    "df_univ[\"univSystem\"] = df_univ[\"university\"].str.contains(\"System\", case=False, regex=False)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# FLAG: Duplicated names requiring removal (duplic==1)\n",
    "# ----------------------------------------------------\n",
    "duplic_patterns = [\n",
    "    \"University of London\",\n",
    "    \"University of North Carolina\",\n",
    "    \"University of Washington\",\n",
    "    \"Pennsylvania State University\",\n",
    "    \"Arizona State University\",\n",
    "    \"Indiana University Bloomington\",\n",
    "    \"Nanyang Technological University & National Institute of Education (NIE)\",\n",
    "    \"IESE Business School\",\n",
    "    \"University of Illinois Chicago Hospital\",\n",
    "    \"Universite de Montreal\",\n",
    "    \"University of South Carolina\",\n",
    "    \"Mays Business School\",\n",
    "    \"New York University Tandon School of Engineering\",\n",
    "    \"University of North Carolina School of Medicine\",\n",
    "    \"\",\n",
    "    \"Erasmus University Rotterdam - Excl Erasmus MC\"\n",
    "]\n",
    "\n",
    "pattern_regex = \"|\".join([f\"^{p}$\" if p else r\"^$\" for p in duplic_patterns])\n",
    "df_univ[\"duplic\"] = df_univ[\"university\"].str.match(pattern_regex, case=False)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# FLAG: Centers (centers == 1)\n",
    "# ----------------------------------------------------\n",
    "center_patterns = [\n",
    "    \"National Bureau of Economic Research\",\n",
    "    \"Federal Reserve Bank\",\n",
    "    \"Centre for Economic Policy Research - UK\",\n",
    "    \"Center for Economic & Policy Research (CEPR)\",\n",
    "    \"UDICE-French Research Universities - investigate\",\n",
    "    \"Centre National de la Recherche Scientifique (CNRS)\",\n",
    "    \"Chinese Academy of Sciences\",\n",
    "    \"European Corporate Governance Institute\",\n",
    "    \"Swiss Federal Institutes of Technology Domain\"\n",
    "]\n",
    "\n",
    "df_univ[\"centers\"] = df_univ[\"university\"].apply(\n",
    "    lambda x: any(p.lower() in x.lower() for p in center_patterns)\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# APPLY STATA DROPS\n",
    "# ----------------------------------------------------\n",
    "df_univ = df_univ[df_univ[\"univSystem\"] != True]\n",
    "df_univ = df_univ[df_univ[\"duplic\"] != True]\n",
    "df_univ = df_univ[df_univ[\"centers\"] != True]\n",
    "df_univ = df_univ[df_univ[\"university\"] != \"Microsoft\"]\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# LIST OF BANK / CENTRAL BANK AFFILIATIONS TO REMOVE\n",
    "# --------------------------------------------------------\n",
    "BANK_NAMES = [\n",
    "    \"European Central Bank\",\n",
    "    \"The World Bank\",\n",
    "    \"Bank of Canada\",\n",
    "    \"Bank of Italy\",\n",
    "    \"Bank for International Settlements (BIS)\",\n",
    "    \"Deutsche Bundesbank\",\n",
    "    \"Bank of England\",\n",
    "    \"Bank of France\",\n",
    "    \"Bank of Finland\",\n",
    "    \"Egyptian Knowledge Bank (EKB)\",\n",
    "    \"Inter-American Development Bank\",\n",
    "    \"De Nederlandsche Bank NV\",\n",
    "    \"Swiss National Bank (SNB)\",\n",
    "    \"Norges Bank\",\n",
    "    \"Sveriges Riksbank\",\n",
    "    \"Central Bank of Chile\",\n",
    "    \"National Bank of Slovakia\",\n",
    "    \"Central Bank of Ireland\",\n",
    "    \"Central Bank of Brazil\",\n",
    "    \"European Bank of Reconstructon & Development\",\n",
    "    \"Reserve Bank of India\",\n",
    "    \"National Bank of Belgium\",\n",
    "    \"Bank of Israel\",\n",
    "    \"Bank of Mexico\",\n",
    "    \"Bank of Lithuania\",\n",
    "    \"Bank of America Corporation\",\n",
    "    \"National Bank of Ukraine\",\n",
    "    \"Bank of Japan\",\n",
    "    \"African Development Bank Group (AfDB)\",\n",
    "    \"Industrial & Commercial Bank of China (ICBC)\",\n",
    "    \"London South Bank University\",\n",
    "    \"University of Alaska Fairbanks\",\n",
    "    \"Banking Academy of Vietnam\",\n",
    "    \"Central Bank of the Republic of Turkey\",\n",
    "    \"Commerzbank AG\",\n",
    "    \"Bank of Communications\",\n",
    "    \"Yapi Kredi Bank\",\n",
    "    \"Oesterreichische Nationalbank (OeNB)\",\n",
    "    \"People's Bank of China\",\n",
    "    \"Bank of Korea\"\n",
    "]\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# REMOVE BANKS / CENTRAL BANKS FROM UNIVERSITY LIST\n",
    "# --------------------------------------------------------\n",
    "df_univ = df_univ[~df_univ[\"university\"].isin(BANK_NAMES)]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# DROP helper columns\n",
    "# ----------------------------------------------------\n",
    "df_univ = df_univ.drop(columns=[\"univSystem\", \"duplic\", \"centers\"])\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# LONG FORMAT: Paper × Author\n",
    "# ----------------------------------------------------\n",
    "rows_auth = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    for author in row[\"author_list\"]:\n",
    "        rows_auth.append({\n",
    "            \"paper_id\": int(idx),\n",
    "            \"title\": row[\"title\"],\n",
    "            \"year\": int(row[\"year\"]),\n",
    "            \"journal\": row[\"journalFullName\"],\n",
    "            \"author\": author,\n",
    "            \"disciplineAbbr\": row[\"disciplineAbbr\"],\n",
    "            \"utd24\": int(row[\"utd24\"]),\n",
    "            \"ft50\": int(row[\"ft50\"])\n",
    "        })\n",
    "\n",
    "df_auth = pd.DataFrame(rows_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55bc15a5-120a-4e50-aec7-80a3a467a2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning complete. Final rows: 171697\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# LOAD DATA\n",
    "# -----------------------------\n",
    "\n",
    "lookup = pd.read_stata(\"authorsLookUpFT50_30Oct2025_v1gz.dta\")\n",
    "\n",
    "# Make sure columns we use exist:\n",
    "# df columns: paper_id, title, year, journal, author, disciplineAbbr, utd24, ft50\n",
    "# lookup columns: authnm, authnm2 (check this!)\n",
    "# rename `author` → `authnm` to follow STATA flow\n",
    "df_auth = df_auth.rename(columns={\"author\": \"authnm\"})\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# STATA: replace authnm = subinstr(authnm, \" \", \"\", 1) \n",
    "#        if substr(authnm,1,1) == \" \"\n",
    "# Meaning: Remove ONE leading space\n",
    "# ======================================================\n",
    "\n",
    "df_auth[\"authnm\"] = df_auth[\"authnm\"].apply(lambda x: x[1:] if isinstance(x, str) and x.startswith(\" \") else x)\n",
    "\n",
    "# ======================================================\n",
    "# STATA: drop if authnm == \"\"\n",
    "# ======================================================\n",
    "\n",
    "df_auth = df_auth[df_auth[\"authnm\"].astype(str).str.strip() != \"\"]\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# STATA: by ut authnm, sort: keep if _n == _N\n",
    "# ut = unique paper_id\n",
    "# Keep last duplicate per (paper_id, authnm)\n",
    "# ======================================================\n",
    "\n",
    "df_auth = df_auth.sort_values([\"paper_id\", \"authnm\"])\n",
    "df_auth = df_auth.drop_duplicates(subset=[\"paper_id\", \"authnm\"], keep=\"last\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# STATA: merge m:1 authnm using lookup\n",
    "# ======================================================\n",
    "\n",
    "lookup = lookup.rename(columns=str.lower)   # normalize names\n",
    "df_auth[\"authnm\"] = df_auth[\"authnm\"].astype(str)\n",
    "\n",
    "merged = df_auth.merge(lookup, on=\"authnm\", how=\"left\", indicator=True)\n",
    "\n",
    "# STATA: drop if _merge == 2 (lookup-only rows)\n",
    "merged = merged[merged[\"_merge\"] != \"right_only\"]\n",
    "\n",
    "# STATA: replace authnm2 = authnm if authnm2 == \"\"\n",
    "merged[\"authnm2\"] = merged[\"authnm2\"].fillna(\"\")\n",
    "mask_empty = merged[\"authnm2\"].astype(str).str.strip() == \"\"\n",
    "merged.loc[mask_empty, \"authnm2\"] = merged.loc[mask_empty, \"authnm\"]\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Special substitutions (STATA)\n",
    "# ======================================================\n",
    "\n",
    "merged[\"authnm2\"] = merged[\"authnm2\"].replace({\n",
    "    \"Gupt, Alok\": \"Gupta, Alok\",\n",
    "    \"Tucker, Catherine Elizabeth\": \"Tucker, Catherine E.\"\n",
    "})\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# STATA: drop if authnm2 == \"\"\n",
    "# ======================================================\n",
    "\n",
    "merged = merged[merged[\"authnm2\"].astype(str).str.strip() != \"\"]\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# STATA: drop if authnm2 == \"[Anonymous]\"\n",
    "# ======================================================\n",
    "\n",
    "merged = merged[merged[\"authnm2\"] != \"[Anonymous]\"]\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# STATA: by ut authnm2, sort: keep if _n == _N\n",
    "# (remove duplicate authors on the same paper)\n",
    "# ======================================================\n",
    "\n",
    "merged = merged.sort_values([\"paper_id\", \"authnm2\"])\n",
    "merged = merged.drop_duplicates(subset=[\"paper_id\", \"authnm2\"], keep=\"last\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Rename cleaned author name back to \"author\" \n",
    "# ======================================================\n",
    "\n",
    "merged = merged.rename(columns={\"authnm2\": \"author\"})\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# EXPORT CLEANED FILE\n",
    "# ======================================================\n",
    "merged = merged.drop(columns=[\"authnm\", \"_merge\"])\n",
    "merged.to_csv(\"paper_author_long_CLEANED.csv\", index=False)\n",
    "df_auth = merged\n",
    "print(\"Cleaning complete. Final rows:\", len(merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "132a186f-0cf0-4d51-8531-06bcf731fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_univ.to_csv(\"paper_university_long.csv\", index=False)\n",
    "df_auth.to_csv(\"paper_author_long.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e2a0d1-e413-41c1-9063-8061d2e5ed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_univ.to_json(\"universities.json\", orient=\"records\")\n",
    "df_auth.to_json(\"authors.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be5eb134-5bbd-4388-ba83-d081e01c5de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset_univ = df_univ[df_univ[\"year\"] >= 2016]\n",
    "#subset_auth = df_auth[df_auth[\"year\"] >= 2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f55415b4-941c-461b-8c92-f2c2f173dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset_univ.to_json(\"universitiesSub.json\", orient=\"records\")\n",
    "#subset_auth.to_json(\"authorsSub.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64fbb5f8-0869-4e4a-a347-132361ec46a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Saved valid JSON: articlesSub.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Excel\n",
    "df = pd.read_excel(\"51journals_UTD_FT_openalexWOSdoi_utdft.xlsx\")\n",
    "\n",
    "# Standardize column names\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Rename columns if needed\n",
    "rename_map = {\n",
    "    \"journal_fullname\": \"journal\",\n",
    "    \"journalfullname\": \"journal\",\n",
    "    \"journal_full_name\": \"journal\",\n",
    "    \"disciplineabbr\": \"disciplineabbr\",\n",
    "}\n",
    "df = df.rename(columns=rename_map)\n",
    "\n",
    "# Keep only necessary columns\n",
    "df = df[\n",
    "    [\"title\", \"year\", \"journal\", \"authors\", \"affiliations\",\n",
    "     \"disciplineabbr\", \"utd24\", \"ft50\"]\n",
    "]\n",
    "\n",
    "# Filter years\n",
    "#df = df[(df[\"year\"] >= 2016) & (df[\"year\"] <= 2024)]\n",
    "\n",
    "# Drop rows missing essentials\n",
    "df = df.dropna(subset=[\"title\", \"year\", \"journal\"]).reset_index(drop=True)\n",
    "\n",
    "# ✅ THIS IS THE KEY LINE — NaN → null automatically\n",
    "#df.to_json(\"articlesSub.json\", orient=\"records\")\n",
    "df.to_json(\"articles.json\", orient=\"records\")\n",
    "\n",
    "print(\"✔ Saved valid JSON: articlesSub.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172391f6-3564-4064-a672-2876ece55126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
